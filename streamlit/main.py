import streamlit as st
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, KFold, GridSearchCV, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc, precision_recall_curve
import plotly.figure_factory as ff
import plotly.graph_objects as go
import time
from tqdm import tqdm
from pygwalker.api.streamlit import StreamlitRenderer
import xgboost as xgb
import lightgbm as lgb
import shap

st.set_page_config(page_title="@data_lemak", layout="wide")
st.sidebar.title("è¨­ç½®")

with st.sidebar.expander("OpenAI API Settings (Coming Soon)"):
    openai_apikey = st.text_input("Enter OpenAI API Key", type="password")
    if openai_apikey:
        st.success("API Key entered!")
    temperature = st.slider("Temperature", min_value=0.0, max_value=1.0, value=0.7, step=0.1)
    openai_model = st.selectbox("OpenAI Model", ["gpt-3.5-turbo", "gpt-4", "gpt-4o"])


st.sidebar.divider()
st.sidebar.header("å°ˆæ¡ˆæµç¨‹")
sidebar_options = [
    "1. åŠ è¼‰æ•¸æ“š",
    "2. æ•¸æ“šæŽ¢ç´¢å’Œå¯è¦–åŒ–",
    "3. æ•¸æ“šé è™•ç†",
    "4. ç‰¹å¾µå·¥ç¨‹",
    "5. æ¨¡åž‹é¸æ“‡èˆ‡è¨“ç·´",
    "6. æ¨¡åž‹è©•ä¼°",
    "7. æ¨¡åž‹éƒ¨ç½²æ¨¡æ“¬",
    "8. What-If åˆ†æž"
]
selected_option = st.sidebar.radio("é¸æ“‡æ­¥é©Ÿ(Demo)", sidebar_options)
st.title("ðŸ“Š ç”¨æˆ¶æµå¤±é æ¸¬")
st.header("1. Data Loading")
data_option = st.radio("é¸æ“‡æ•¸æ“šä¾†æº", ["ä¸Šå‚³CSVæ–‡ä»¶", "éš¨æ©Ÿè³‡æ–™"])

def load_sample_data(n_samples=1000):
    np.random.seed(42)
    df = pd.DataFrame({
        'age': np.random.randint(18, 80, n_samples),
        'tenure': np.random.randint(0, 60, n_samples),
        'balance': np.random.uniform(0, 250000, n_samples),
        'num_products': np.random.randint(1, 5, n_samples),
        'has_credit_card': np.random.choice([0, 1], n_samples),
        'is_active_member': np.random.choice([0, 1], n_samples),
        'estimated_salary': np.random.uniform(30000, 200000, n_samples),
        'churn': np.random.choice([0, 1], n_samples, p=[0.8, 0.2])
    })
    return df

if data_option == "ä¸Šå‚³CSVæ–‡ä»¶":
    uploaded_file = st.file_uploader("ä¸Šå‚³CSVæ–‡ä»¶", type="csv")
    if uploaded_file is not None:
        df = pd.read_csv(uploaded_file)
        st.success("æˆåŠŸè¼‰å…¥ä¸Šå‚³çš„CSVæ–‡ä»¶ï¼")
        st.session_state.df = df
    else:
        st.warning("Please upload a CSV file @data_lemak")
        st.stop()
else:
    n_samples = st.number_input("è¨­å®šæ¨£æœ¬æ•¸", min_value=1000, value=1000, step=50)
    df = load_sample_data(n_samples)
    st.success("Successfully Loaded Dataset! @data_lemak")
    st.session_state.df = df

st.write(st.session_state.df.head())

st.header("2. Data Exploration and Visualization")
with st.expander("å±•é–‹ä»¥æŸ¥çœ‹æ•¸æ“šæŽ¢ç´¢"):
    st.write("ä½¿ç”¨ PyGWalker é€²è¡Œæ•¸æ“šæŽ¢ç´¢:")

    if 'df' in st.session_state:
        vis_spec = r"""{"config":[{"config":{"defaultAggregated":true,"geoms":["auto"],"coordSystem":"generic","limit":-1,"timezoneDisplayOffset":0,"folds":["tenure"]},"encodings":{"dimensions":[{"fid":"num_products","name":"num_products","basename":"num_products","semanticType":"quantitative","analyticType":"dimension","offset":0},{"fid":"churn","name":"churn","basename":"churn","semanticType":"quantitative","analyticType":"dimension","offset":0},{"fid":"age","name":"age","basename":"age","analyticType":"dimension","semanticType":"quantitative","aggName":"sum","offset":0},{"fid":"has_credit_card","name":"has_credit_card","basename":"has_credit_card","semanticType":"quantitative","analyticType":"dimension","offset":0},{"fid":"is_active_member","name":"is_active_member","basename":"is_active_member","semanticType":"quantitative","analyticType":"dimension","offset":0},{"fid":"gw_mea_key_fid","name":"Measure names","analyticType":"dimension","semanticType":"nominal"}],"measures":[{"fid":"tenure","name":"tenure","basename":"tenure","analyticType":"measure","semanticType":"quantitative","aggName":"sum","offset":0},{"fid":"balance","name":"balance","basename":"balance","analyticType":"measure","semanticType":"quantitative","aggName":"sum","offset":0},{"fid":"estimated_salary","name":"estimated_salary","basename":"estimated_salary","semanticType":"measure","analyticType":"quantitative","aggName":"sum","offset":0},{"fid":"gw_count_fid","name":"Row count","analyticType":"measure","semanticType":"quantitative","aggName":"sum","computed":true,"expression":{"op":"one","params":[],"as":"gw_count_fid"}},{"fid":"gw_mea_val_fid","name":"Measure values","analyticType":"measure","semanticType":"quantitative","aggName":"sum"}],"rows":[{"fid":"gw_count_fid","name":"Row count","analyticType":"measure","semanticType":"quantitative","aggName":"sum","computed":true,"expression":{"op":"one","params":[],"as":"gw_count_fid"}}],"columns":[{"fid":"age","name":"age","basename":"age","analyticType":"dimension","semanticType":"quantitative","aggName":"sum","offset":0}],"color":[],"opacity":[],"size":[],"shape":[],"radius":[],"theta":[],"longitude":[],"latitude":[],"geoId":[],"details":[{"fid":"tenure","name":"tenure","basename":"tenure","analyticType":"measure","semanticType":"quantitative","aggName":"mean","offset":0},{"fid":"estimated_salary","name":"estimated_salary","basename":"estimated_salary","analyticType":"measure","semanticType":"quantitative","aggName":"mean","offset":0}],"filters":[],"text":[]},"layout":{"showActions":false,"showTableSummary":false,"stack":"stack","interactiveScale":false,"zeroScale":true,"size":{"mode":"full","width":320,"height":200},"format":{},"geoKey":"name","resolve":{"x":false,"y":false,"color":false,"opacity":false,"shape":false,"size":false}},"visId":"gw_fRWT","name":"Chart 1"}],"chart_map":{},"workflow_list":[{"workflow":[{"type":"transform","transform":[{"key":"gw_count_fid","expression":{"op":"one","params":[],"as":"gw_count_fid"}}]},{"type":"view","query":[{"op":"aggregate","groupBy":["age"],"measures":[{"field":"gw_count_fid","agg":"sum","asFieldKey":"gw_count_fid_sum"},{"field":"tenure","agg":"mean","asFieldKey":"tenure_mean"},{"field":"estimated_salary","agg":"mean","asFieldKey":"estimated_salary_mean"}]}]}]}],"version":"0.4.9.7"}"""
        pyg_app = StreamlitRenderer(df, vis_spec=vis_spec)
        pyg_app.explorer()
    else:
        st.warning("Please load the data first! @data_lemak")

st.header("3. Data Preprocessing")
with st.expander("å±•é–‹ä»¥æŸ¥çœ‹æ•¸æ“šé è™•ç†"):
    st.write("Missing Value Imputation")
    fill_methods = {col: st.selectbox(f"é¸æ“‡ {col} çš„å¡«è£œæ–¹å¼", ['mean', 'mode', '0'], key=f"fill_{col}") for col in st.session_state.df.columns}

    if st.button("åŸ·è¡Œæ•¸æ“šé è™•ç†"):
        progress_text = st.empty()
        progress_bar = st.progress(0)
        for i, column in tqdm(enumerate(st.session_state.df.columns), desc="åŸ·è¡Œæ•¸æ“šé è™•ç†", total=len(st.session_state.df.columns), ncols=100, bar_format="{l_bar}{bar} [æ™‚é–“å‰©é¤˜: {remaining}] @data_lemak"):
            progress_text.text(f"æ­£åœ¨è™•ç†: {column}")
            if st.session_state.df[column].dtype != 'object':
                if fill_methods[column] == 'mean':
                    st.session_state.df[column] = st.session_state.df[column].fillna(st.session_state.df[column].mean())
                elif fill_methods[column] == 'mode':
                    st.session_state.df[column] = st.session_state.df[column].fillna(st.session_state.df[column].mode()[0])
                else:
                    st.session_state.df[column] = st.session_state.df[column].fillna(0)
            else:
                st.session_state.df[column] = st.session_state.df[column].fillna(st.session_state.df[column].mode()[0])
            progress_bar.progress((i + 1) / len(st.session_state.df.columns))
            time.sleep(0.1)  
        progress_text.text("æ•¸æ“šé è™•ç†å®Œæˆï¼")
        st.success("Data preprocessing completed! @data_lemak")
        st.write(st.session_state.df.head())

# Feature Engineering
st.header("4. Feature Engineering")
with st.expander("å±•é–‹ä»¥æŸ¥çœ‹ç‰¹å¾µå·¥ç¨‹"):
    features_to_process = ['age', 'tenure', 'balance', 'num_products', 'has_credit_card', 'is_active_member', 'estimated_salary']

    feature_bins = {}
    for feature in features_to_process:
        if st.session_state.df[feature].dtype != 'object':
            st.subheader(f"{feature} åˆ†çµ„è¨­ç½®")
            num_bins = st.slider(f"Number of bins for {feature}", 2, 10, 4, key=f"bins_{feature}")
            min_val, max_val = float(st.session_state.df[feature].min()), float(st.session_state.df[feature].max())
            bins = st.slider(f"Bin range for {feature}", min_val, max_val, (min_val, max_val), key=f"range_{feature}")
            feature_bins[feature] = np.linspace(bins[0], bins[1], num_bins + 1)

    def process_feature(feature, feature_name):
        if feature_name in ['has_credit_card', 'is_active_member']:
            return feature.map({0: 'No', 1: 'Yes'})
        elif feature_name in feature_bins:
            return pd.cut(feature, bins=feature_bins[feature_name], labels=[f'bin_{i+1}' for i in range(len(feature_bins[feature_name])-1)])
        else:
            return feature

    if st.button("åŸ·è¡Œç‰¹å¾µå·¥ç¨‹"):
        progress_text = st.empty()
        progress_bar = st.progress(0)
        for i, feature in tqdm(enumerate(features_to_process), desc="åŸ·è¡Œç‰¹å¾µå·¥ç¨‹", total=len(features_to_process), ncols=100, bar_format="{l_bar}{bar} [æ™‚é–“å‰©é¤˜: {remaining}] @data_lemak"):
            progress_text.text(f"æ­£åœ¨è™•ç†: {feature}")
            st.session_state.df[feature + '_processed'] = process_feature(st.session_state.df[feature], feature)
            progress_bar.progress((i + 1) / len(features_to_process))
            time.sleep(0.1)
        progress_text.text("ç‰¹å¾µå·¥ç¨‹å®Œæˆï¼")
        st.success("Feature engineering completed! @data_lemak")
        st.write(st.session_state.df.head())

# Select features
st.header("5. Select Features for Training")
features_for_model = st.multiselect(
    "é¸æ“‡ç‰¹å¾µ",
    options=[col for col in st.session_state.df.columns if col.endswith('_processed') or col in features_to_process],
    default=[col for col in st.session_state.df.columns if col.endswith('_processed') or col in features_to_process]
)

# Model Selection and Training
st.header("6. Model Selection and Training")
tuning_method = st.radio("é¸æ“‡è¶…åƒæ•¸èª¿å„ªæ–¹æ³•", ["æ‰‹å‹•èª¿æ•´", "GridSearchCV", "RandomizedSearchCV"])
if tuning_method == "æ‰‹å‹•èª¿æ•´":
    n_splits = st.slider("K-Fold äº¤å‰é©—è­‰æŠ˜æ•¸", 2, 10, 5)
model_option = st.selectbox("é¸æ“‡è¦ä½¿ç”¨çš„æ¨¡åž‹", ["Random Forest", "XGBoost", "LightGBM"])

def get_model_params(model_option):
    if model_option == "Random Forest":
        return {
            'n_estimators': st.slider("æ¨¹çš„æ•¸é‡", 10, 1000, 100),
            'max_depth': st.slider("æœ€å¤§æ·±åº¦", 1, 50, 10),
            'min_samples_split': st.slider("æœ€å°åˆ†è£‚æ¨£æœ¬æ•¸", 2, 20, 2),
            'min_samples_leaf': st.slider("æœ€å°è‘‰å­æ¨£æœ¬æ•¸", 1, 20, 1),
            'max_features': st.selectbox("æœ€å¤§ç‰¹å¾µæ•¸", ["sqrt", "log2", None])
        }
    elif model_option == "XGBoost":
        return {
            'n_estimators': st.slider("æ¨¹çš„æ•¸é‡", 10, 1000, 100),
            'learning_rate': st.slider("å­¸ç¿’çŽ‡", 0.01, 1.0, 0.1),
            'max_depth': st.slider("æœ€å¤§æ·±åº¦", 1, 20, 6),
            'min_child_weight': st.slider("æœ€å°å­æ¬Šé‡", 1, 10, 1),
            'gamma': st.slider("ä¿®å‰ªé–¾å€¼", 0.0, 1.0, 0.0, 0.1),
            'subsample': st.slider("å­æŽ¡æ¨£æ¯”ä¾‹", 0.1, 1.0, 1.0, 0.1),
            'colsample_bytree': st.slider("ç‰¹å¾µæŽ¡æ¨£æ¯”ä¾‹", 0.1, 1.0, 1.0, 0.1)
        }
    else:  # LightGBM
        return {
            'n_estimators': st.slider("æ¨¹çš„æ•¸é‡", 10, 1000, 100),
            'learning_rate': st.slider("å­¸ç¿’çŽ‡", 0.01, 1.0, 0.1),
            'num_leaves': st.slider("è‘‰å­æ•¸é‡", 20, 3000, 31),
            'max_depth': st.slider("æœ€å¤§æ·±åº¦", -1, 20, -1),
            'min_child_samples': st.slider("æœ€å°å­æ¨£æœ¬æ•¸", 1, 100, 20),
            'subsample': st.slider("å­æŽ¡æ¨£æ¯”ä¾‹", 0.1, 1.0, 1.0, 0.1),
            'colsample_bytree': st.slider("ç‰¹å¾µæŽ¡æ¨£æ¯”ä¾‹", 0.1, 1.0, 1.0, 0.1)
        }
def get_model(model_option, model_params):
    if model_option == "Random Forest":
        return RandomForestClassifier(**model_params, random_state=42, n_jobs=-1)
    elif model_option == "XGBoost":
        return xgb.XGBClassifier(**model_params, random_state=42, n_jobs=-1)
    else:  # LightGBM
        return lgb.LGBMClassifier(**model_params, random_state=42, n_jobs=-1)

if tuning_method == "æ‰‹å‹•èª¿æ•´":
    model_params = get_model_params(model_option)
    model = get_model(model_option, model_params)

    if st.button("è¨“ç·´æ¨¡åž‹"):
        X = st.session_state.df[features_for_model]
        y = st.session_state.df['churn']
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        progress_bar = st.progress(0)
        progress_text = st.empty()
        cv_scores = []
        kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)

        start_time = time.time()
        with tqdm(total=n_splits, desc="ç¸½é«”é€²åº¦") as pbar_outer:
            for fold, (train_index, val_index) in enumerate(kf.split(X_train_scaled)):
                X_train_cv, X_val_cv = X_train_scaled[train_index], X_train_scaled[val_index]
                y_train_cv, y_val_cv = y_train.iloc[train_index], y_train.iloc[val_index]
                with tqdm(total=100, desc=f"Fold {fold+1}/{n_splits}  by @data_lemak", leave=False) as pbar_inner:
                    for i in range(10):  # æ¨¡æ“¬è¨“ç·´éŽç¨‹
                        model.fit(X_train_cv, y_train_cv)
                        pbar_inner.update(10)
                        time.sleep(0.01)  # æ¨¡æ“¬è¨“ç·´æ™‚é–“
                score = model.score(X_val_cv, y_val_cv)
                cv_scores.append(score)
                pbar_outer.update(1)
                progress = (fold + 1) / n_splits
                progress_bar.progress(progress)
                progress_text.text(f"ç¸½é«”é€²åº¦: {progress*100:.0f}% (Fold {fold+1}/{n_splits}) by @data_lemak")

        mean_cv_score = np.mean(cv_scores)
        with tqdm(total=100, desc="æœ€çµ‚æ¨¡åž‹è¨“ç·´  by @data_lemak") as pbar_final:
            for i in range(10):  # æ¨¡æ“¬æœ€çµ‚è¨“ç·´éŽç¨‹
                model.fit(X_train_scaled, y_train)
                pbar_final.update(10)
                time.sleep(0.01)  # æ¨¡æ“¬è¨“ç·´æ™‚é–“

        end_time = time.time()
        training_time = end_time - start_time

        y_pred = model.predict(X_test_scaled)
        y_prob = model.predict_proba(X_test_scaled)[:, 1]

        st.session_state.update({
            'model': model,
            'X_test': X_test,
            'X_test_scaled': X_test_scaled,
            'y_test': y_test,
            'y_pred': y_pred,
            'y_prob': y_prob,
            'scaler': scaler,
            'features': X.columns,
            'cv_scores': cv_scores,
            'training_time': training_time,
            'num_samples': len(X)
        })

        st.success("Model training completed! You can proceed with model evaluation and interpretation. @data_lemak")

elif tuning_method in ["GridSearchCV", "RandomizedSearchCV"]:
    param_grid = {
        'Random Forest': {
            'n_estimators': [100, 200, 300],
            'max_depth': [10, 20, 30],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4],
            'max_features': ['sqrt', 'log2']
        },
        'XGBoost': {
            'n_estimators': [100, 200, 300],
            'learning_rate': [0.01, 0.1, 0.2],
            'max_depth': [6, 10, 15],
            'min_child_weight': [1, 3, 5],
            'gamma': [0, 0.1, 0.2],
            'subsample': [0.8, 0.9, 1.0],
            'colsample_bytree': [0.8, 0.9, 1.0]
        },
        'LightGBM': {
            'n_estimators': [100, 200, 300],
            'learning_rate': [0.01, 0.1, 0.2],
            'num_leaves': [31, 50, 100],
            'max_depth': [-1, 10, 20],
            'min_child_samples': [20, 30, 40],
            'subsample': [0.8, 0.9, 1.0],
            'colsample_bytree': [0.8, 0.9, 1.0]
        }
    }

    model_params = get_model_params(model_option)
    model = get_model(model_option, model_params)
    search_method = GridSearchCV if tuning_method == "GridSearchCV" else RandomizedSearchCV
    search = search_method(model, param_grid[model_option], cv=3, n_jobs=-1)

    if st.button("è¨“ç·´æ¨¡åž‹"):
        X = st.session_state.df[features_for_model]
        y = st.session_state.df['churn']
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        progress_bar = st.progress(0)
        progress_text = st.empty()

        start_time = time.time()
        n_iter = 10  # æ¨¡æ“¬æœç´¢éŽç¨‹çš„è¿­ä»£æ¬¡æ•¸
        with tqdm(total=n_iter, desc="è¶…åƒæ•¸æœç´¢é€²åº¦  by @data_lemak") as pbar_search:
            for i in range(n_iter):
                search.fit(X_train_scaled, y_train)
                progress = (i + 1) / n_iter
                progress_bar.progress(progress)
                progress_text.text(f"è¶…åƒæ•¸æœç´¢é€²åº¦: {progress*100:.0f}% @data_lemak")
                pbar_search.update(1)
                time.sleep(0.1)  # æ¨¡æ“¬æœç´¢æ™‚é–“

        end_time = time.time()
        training_time = end_time - start_time

        best_model = search.best_estimator_

        y_pred = best_model.predict(X_test_scaled)
        y_prob = best_model.predict_proba(X_test_scaled)[:, 1]

        st.session_state.update({
            'model': best_model,
            'X_test': X_test,
            'X_test_scaled': X_test_scaled,
            'y_test': y_test,
            'y_pred': y_pred,
            'y_prob': y_prob,
            'scaler': scaler,
            'features': X.columns,
            'cv_scores': [],  # Clear cv_scores to avoid errors
            'training_time': training_time,
            'num_samples': len(X)
        })

        st.success("Model training completed! You can proceed with model evaluation and interpretation. @data_lemak")

# Model Evaluation
st.header("7. Model Evaluation")

# Add model description
if 'model' in st.session_state:
    model_description = {
        "Random Forest": "éš¨æ©Ÿæ£®æž—æ˜¯ä¸€ç¨®é›†æˆå­¸ç¿’æ–¹æ³•ï¼Œé€šéŽæ§‹å»ºå¤šæ£µæ±ºç­–æ¨¹ä¸¦å°‡å…¶çµæžœé€²è¡Œå¹³å‡ä¾†æé«˜é æ¸¬æº–ç¢ºæ€§å’ŒæŽ§åˆ¶éŽæ“¬åˆã€‚",
        "XGBoost": "XGBoost æ˜¯ä¸€ç¨®å¼·å¤§çš„æ¢¯åº¦æå‡ç®—æ³•ï¼Œé€šéŽé€æ­¥æ·»åŠ æ¨¡åž‹ä¾†æœ€å°åŒ–æå¤±å‡½æ•¸ï¼Œä¸¦ä¸”å…·æœ‰é«˜æ•ˆçš„è¨ˆç®—æ€§èƒ½ã€‚",
        "LightGBM": "LightGBM æ˜¯ä¸€ç¨®åŸºæ–¼æ¨¹çš„å­¸ç¿’ç®—æ³•ï¼Œå°ˆç‚ºé«˜æ•ˆè™•ç†å¤§è¦æ¨¡æ•¸æ“šè€Œè¨­è¨ˆï¼Œå…·æœ‰æ›´å¿«çš„è¨“ç·´é€Ÿåº¦å’Œæ›´ä½Žçš„å…§å­˜ä½¿ç”¨ã€‚"
    }
    st.markdown(f"#### æ¨¡åž‹é¡žåž‹")
    st.markdown(f"```{model_option}```", unsafe_allow_html=True)
    st.markdown(f"```**{model_description.get(model_option, 'æœªçŸ¥æ¨¡åž‹')}**```", unsafe_allow_html=True)

    st.subheader("æ¨¡åž‹åƒæ•¸")
    st.json(st.session_state['model'].get_params())

    col1, col2 = st.columns(2)
    with col1:
        st.subheader("è¨“ç·´æ™‚é–“")
        st.write(f"{st.session_state['training_time']:.2f} ç§’")
    with col2:
        st.subheader("è³‡æ–™ç­†æ•¸")
        st.write(f"{st.session_state['num_samples']} ç­†")

if 'model' in st.session_state:
    if tuning_method == "æ‰‹å‹•èª¿æ•´":
        st.subheader("äº¤å‰é©—è­‰çµæžœ")
        cv_scores = st.session_state.get('cv_scores', [])
        if cv_scores:
            col1, col2 = st.columns(2)
            col1.metric("äº¤å‰é©—è­‰å¹³å‡å¾—åˆ†:", f"{np.mean(cv_scores):.4f}")
            col2.metric("æ¨™æº–å·®", f"{np.std(cv_scores):.4f}")

            color_option = st.selectbox("é¸æ“‡åœ–è¡¨é¡è‰²", ["lightblue", "lightgreen", "lightcoral", "plum", "peachpuff"])
            color_map = {
                "lightblue": "#ADD8E6",
                "lightgreen": "#90EE90",
                "lightcoral": "#F08080",
                "plum": "#DDA0DD",
                "peachpuff": "#FFDAB9"
            }
            selected_color = color_map[color_option]
            fig = go.Figure(data=[go.Bar(y=cv_scores, x=[f"Fold {i+1}" for i in range(len(cv_scores))], marker_color=selected_color)])
            fig.update_layout(title="äº¤å‰é©—è­‰å„æŠ˜å¾—åˆ†", xaxis_title="Fold", yaxis_title="å¾—åˆ†")
            st.plotly_chart(fig)
        else:
            st.write("æœªæ‰¾åˆ°äº¤å‰é©—è­‰çµæžœ ã€‚")

    st.subheader("æœ€çµ‚æ¨¡åž‹è©•ä¼°æŒ‡æ¨™")
    col1, col2, col3, col4 = st.columns(4)
    
    # ä½¿ç”¨ zero_division åƒæ•¸ä¾†é¿å…è­¦å‘Š
    accuracy = accuracy_score(st.session_state['y_test'], st.session_state['y_pred'])
    precision = precision_score(st.session_state['y_test'], st.session_state['y_pred'], zero_division=0)
    recall = recall_score(st.session_state['y_test'], st.session_state['y_pred'], zero_division=0)
    f1 = f1_score(st.session_state['y_test'], st.session_state['y_pred'], zero_division=0)
    
    col1.metric("æº–ç¢ºçŽ‡", f"{accuracy:.4f}")
    col2.metric("ç²¾ç¢ºçŽ‡", f"{precision:.4f}")
    col3.metric("å¬å›žçŽ‡", f"{recall:.4f}")
    col4.metric("F1 åˆ†æ•¸", f"{f1:.4f}")

    # ROC curve and PR curve
    fpr, tpr, _ = roc_curve(st.session_state['y_test'], st.session_state['y_prob'])
    roc_auc = auc(fpr, tpr)
    precision, recall, _ = precision_recall_curve(st.session_state['y_test'], st.session_state['y_prob'])

    plotly_template = st.selectbox("é¸æ“‡ Plotly æ¨¡æ¿", ["plotly", "ggplot2", "seaborn", "simple_white", "plotly_white", "plotly_dark", "presentation", "xgridoff", "ygridoff", "gridon", "none"])

    col1, col2 = st.columns([1, 1], gap="small")

    with col1:
        fig_roc = go.Figure(data=go.Scatter(x=fpr, y=tpr, mode='lines', name='ROC curve'))
        fig_roc.add_shape(type='line', line=dict(dash='dash'), x0=0, x1=1, y0=0, y1=1)
        fig_roc.update_layout(
            title_text='ROC Curve',
            xaxis_title='False Positive Rate',
            yaxis_title='True Positive Rate',
            yaxis=dict(scaleanchor="x", scaleratio=1),
            xaxis=dict(constrain='domain'),
            height=500,
            template=plotly_template
        )
        fig_roc.add_annotation(
            x=0.5, y=0.5,
            text=f'AUC = {roc_auc:.4f}',
            showarrow=False,
            yshift=10
        )
        st.plotly_chart(fig_roc)

    with col2:
        fig_pr = go.Figure(data=go.Scatter(x=recall, y=precision, mode='lines', name='PR curve'))
        fig_pr.update_layout(
            title_text='Precision-Recall Curve',
            xaxis_title='Recall',
            yaxis_title='Precision',
            height=500,
            template=plotly_template
        )
        st.plotly_chart(fig_pr)

    # Confusion Matrix
    st.subheader("æ··æ·†çŸ©é™£")
    cm = confusion_matrix(st.session_state['y_test'], st.session_state['y_pred'])
    colorscale = st.selectbox("é¸æ“‡é¡è‰²æ¯”ä¾‹", ['aggrnyl', 'agsunset', 'algae', 'amp', 'armyrose', 'balance', 'blackbody', 'bluered', 'blues', 'blugrn', 'bluyl', 'brbg', 'brwnyl', 'bugn', 'bupu', 'burg', 'burgyl', 'cividis', 'curl', 'darkmint', 'deep', 'delta', 'dense', 'earth', 'edge', 'electric', 'emrld', 'fall', 'geyser', 'gnbu', 'gray', 'greens', 'greys', 'haline', 'hot', 'hsv', 'ice', 'icefire', 'inferno', 'jet', 'magenta', 'magma', 'matter', 'mint', 'mrybm', 'mygbm', 'oranges', 'orrd', 'oryel', 'oxy', 'peach', 'phase', 'picnic', 'pinkyl', 'piyg', 'plasma', 'plotly3', 'portland', 'prgn', 'pubu', 'pubugn', 'puor', 'purd', 'purp', 'purples', 'purpor', 'rainbow', 'rdbu', 'rdgy', 'rdpu', 'rdylbu', 'rdylgn', 'redor', 'reds', 'solar', 'spectral', 'speed', 'sunset', 'sunsetdark', 'teal', 'tealgrn', 'tealrose', 'tempo', 'temps', 'thermal', 'tropic', 'turbid', 'turbo', 'twilight', 'viridis', 'ylgn', 'ylgnbu', 'ylorbr', 'ylorrd'])
    
    fig = ff.create_annotated_heatmap(cm, x=['Predicted 0', 'Predicted 1'], y=['Actual 0', 'Actual 1'], colorscale=colorscale)
    fig.update_layout(title_text='Confusion Matrix', xaxis_title='Predicted label', yaxis_title='True label')
    st.plotly_chart(fig)

    # SHAP values
    st.subheader("æ¨¡åž‹è§£é‡‹æ€§ (SHAP å€¼)")
    with st.spinner('è¨ˆç®— SHAP å€¼ä¸­...'):
        explainer = shap.TreeExplainer(st.session_state['model'])
        shap_values = explainer.shap_values(st.session_state['X_test_scaled'])
        
        # æª¢æŸ¥ shap_values çš„å½¢ç‹€
        if isinstance(shap_values, list):
            shap_values = shap_values[1]
        elif len(shap_values.shape) == 3:
            shap_values = shap_values[:, :, 1]
        
        shap_df = pd.DataFrame(shap_values, columns=st.session_state['features'])
        shap_importance = shap_df.abs().mean().sort_values(ascending=False)
        
        colorscale = st.selectbox("é¸æ“‡é¡è‰²æ¯”ä¾‹", ['aggrnyl', 'agsunset', 'algae', 'amp', 'armyrose', 'balance', 'blackbody', 'bluered', 'blues', 'blugrn', 'bluyl', 'brbg', 'brwnyl', 'bugn', 'bupu', 'burg', 'burgyl', 'cividis', 'curl', 'darkmint', 'deep', 'delta', 'dense', 'earth', 'edge', 'electric', 'emrld', 'fall', 'geyser', 'gnbu', 'gray', 'greens', 'greys', 'haline', 'hot', 'hsv', 'ice', 'icefire', 'inferno', 'jet', 'magenta', 'magma', 'matter', 'mint', 'mrybm', 'mygbm', 'oranges', 'orrd', 'oryel', 'oxy', 'peach', 'phase', 'picnic', 'pinkyl', 'piyg', 'plasma', 'plotly3', 'portland', 'prgn', 'pubu', 'pubugn', 'puor', 'purd', 'purp', 'purples', 'purpor', 'rainbow', 'rdbu', 'rdgy', 'rdpu', 'rdylbu', 'rdylgn', 'redor', 'reds', 'solar', 'spectral', 'speed', 'sunset', 'sunsetdark', 'teal', 'tealgrn', 'tealrose', 'tempo', 'temps', 'thermal', 'tropic', 'turbid', 'turbo', 'twilight', 'viridis', 'ylgn', 'ylgnbu', 'ylorbr', 'ylorrd'],key="shap_importance")

        fig = go.Figure(go.Bar(
            y=shap_importance.index,
            x=shap_importance.values,
            orientation='h',
            marker=dict(
                color=shap_importance.values,
                colorscale=colorscale,
                colorbar=dict(title="SHAP å€¼")
            )
        ))
        
        fig.update_layout(
            title='ç‰¹å¾µé‡è¦æ€§ (åŸºæ–¼ SHAP å€¼)',
            xaxis_title='å¹³å‡ |SHAP å€¼|',
            yaxis_title='ç‰¹å¾µ',
            height=500,
            width=700
        )
        
        st.plotly_chart(fig)
        st.markdown("""
        #### SHAPå€¼è§£é‡‹

        ##### ä»€éº¼æ˜¯SHAPå€¼ï¼Ÿ

        SHAPå€¼ï¼ˆSHapley Additive exPlanationsï¼‰å¹«åŠ©æˆ‘å€‘ç†è§£æ¯å€‹ç‰¹å¾µå¦‚ä½•å½±éŸ¿æ¨¡åž‹çš„é æ¸¬çµæžœã€‚æƒ³åƒä¸€ä¸‹,æ¯å€‹ç‰¹å¾µéƒ½æ˜¯ä¸€å€‹çƒå“¡,è€ŒSHAPå€¼å°±æ˜¯è©•åˆ†å¡,å‘Šè¨´æˆ‘å€‘æ¯å€‹çƒå“¡å°æ¯”è³½çµæžœçš„è²¢ç»ã€‚

        ##### SHAPå€¼çš„å«ç¾©

        - **æ­£å€¼**ï¼šé€™å€‹ç‰¹å¾µå¢žåŠ äº†å®¢æˆ¶å¯èƒ½æµå¤±çš„æ©Ÿæœƒã€‚
        ä¾‹å¦‚ï¼šå¦‚æžœ"å®¢æˆ¶æœå‹™æ»¿æ„åº¦"çš„SHAPå€¼ç‚ºæ­£,æ„å‘³è‘—è¼ƒä½Žçš„æ»¿æ„åº¦å¢žåŠ äº†å®¢æˆ¶æµå¤±çš„å¯èƒ½æ€§ã€‚

        - **è² å€¼**ï¼šé€™å€‹ç‰¹å¾µæ¸›å°‘äº†å®¢æˆ¶å¯èƒ½æµå¤±çš„æ©Ÿæœƒã€‚
        ä¾‹å¦‚ï¼šå¦‚æžœ"ä½¿ç”¨æ™‚é•·"çš„SHAPå€¼ç‚ºè² ,æ„å‘³è‘—ä½¿ç”¨æ™‚é–“è¶Šé•·,å®¢æˆ¶è¶Šä¸å¯èƒ½æµå¤±ã€‚
                    
        ##### SHAPå€¼çµ±è¨ˆä¿¡æ¯

        ä»¥ä¸‹æ˜¯SHAPå€¼çš„çµ±è¨ˆä¿¡æ¯,å¹«åŠ©æˆ‘å€‘äº†è§£æ¯å€‹ç‰¹å¾µå½±éŸ¿çš„ç¯„åœå’Œæ•´é«”è¶¨å‹¢ï¼š
        """)

        st.write(shap_df.describe())

        st.markdown("""
        - **å¹³å‡å€¼**ï¼šç‰¹å¾µé€šå¸¸çš„å½±éŸ¿ç¨‹åº¦
        - **æœ€å°å€¼å’Œæœ€å¤§å€¼**ï¼šç‰¹å¾µå½±éŸ¿çš„æ¥µç«¯æƒ…æ³
        - **25%ã€50%ã€75%åˆ†ä½æ•¸**ï¼šç‰¹å¾µå½±éŸ¿çš„å¸¸è¦‹ç¯„åœ
        """)
    
    if hasattr(st.session_state['model'], 'feature_importances_'):
        st.subheader("ç‰¹å¾µé‡è¦æ€§")
        feature_importance = pd.DataFrame({
            'feature': st.session_state['features'],
            'importance': st.session_state['model'].feature_importances_
        }).sort_values('importance', ascending=False)
        
        chart_type = st.selectbox("é¸æ“‡åœ–è¡¨é¡žåž‹", ['Horizontal Bar', 'Vertical Bar'], key="feature_importance_chart_type")
        colorscale = st.selectbox("é¸æ“‡é¡è‰²æ¯”ä¾‹", ['aggrnyl', 'agsunset', 'algae', 'amp', 'armyrose', 'balance', 'blackbody', 'bluered', 'blues', 'blugrn', 'bluyl', 'brbg', 'brwnyl', 'bugn', 'bupu', 'burg', 'burgyl', 'cividis', 'curl', 'darkmint', 'deep', 'delta', 'dense', 'earth', 'edge', 'electric', 'emrld', 'fall', 'geyser', 'gnbu', 'gray', 'greens', 'greys', 'haline', 'hot', 'hsv', 'ice', 'icefire', 'inferno', 'jet', 'magenta', 'magma', 'matter', 'mint', 'mrybm', 'mygbm', 'oranges', 'orrd', 'oryel', 'oxy', 'peach', 'phase', 'picnic', 'pinkyl', 'piyg', 'plasma', 'plotly3', 'portland', 'prgn', 'pubu', 'pubugn', 'puor', 'purd', 'purp', 'purples', 'purpor', 'rainbow', 'rdbu', 'rdgy', 'rdpu', 'rdylbu', 'rdylgn', 'redor', 'reds', 'solar', 'spectral', 'speed', 'sunset', 'sunsetdark', 'teal', 'tealgrn', 'tealrose', 'tempo', 'temps', 'thermal', 'tropic', 'turbid', 'turbo', 'twilight', 'viridis', 'ylgn', 'ylgnbu', 'ylorbr', 'ylorrd'], key="feature_importance_colorscale")

        if chart_type == 'Horizontal Bar':
            fig = go.Figure(go.Bar(
                y=feature_importance['feature'],
                x=feature_importance['importance'],
                orientation='h',
                marker=dict(
                    color=feature_importance['importance'],
                    colorscale=colorscale,
                    colorbar=dict(title="é‡è¦æ€§")
                )
            ))
        elif chart_type == 'Vertical Bar':
            fig = go.Figure(go.Bar(
                x=feature_importance['feature'],
                y=feature_importance['importance'],
                orientation='v',
                marker=dict(
                    color=feature_importance['importance'],
                    colorscale=colorscale,
                    colorbar=dict(title="é‡è¦æ€§")
                )
            ))

        fig.update_layout(
            title='ç‰¹å¾µé‡è¦æ€§',
            xaxis_title='é‡è¦æ€§' if chart_type == 'Vertical Bar' else 'ç‰¹å¾µ',
            yaxis_title='ç‰¹å¾µ' if chart_type == 'Vertical Bar' else 'é‡è¦æ€§',
            height=500,
            width=700
        )

        st.plotly_chart(fig)

else:
    st.warning("Please train the model first! @data_lemak")

# Model Deployment Simulation
st.header("8. Model Deployment Simulation")
st.write("è¼¸å…¥æ–°çš„æ•¸æ“šï¼Œçœ‹çœ‹æ¨¡åž‹çš„é æ¸¬çµæžœï¼š")

if 'features' in st.session_state:
    input_data = {}
    for feature in st.session_state['features']:
        if st.session_state.df[feature].dtype == 'object':
            input_data[feature] = st.selectbox(f"Select {feature}", st.session_state.df[feature].unique())
        else:
            input_data[feature] = st.number_input(f"Enter {feature}", value=float(st.session_state.df[feature].mean()))

    if 'prediction_value' not in st.session_state:
        st.session_state['prediction_value'] = None

    if st.button("é æ¸¬"):
        if 'model' in st.session_state:
            input_df = pd.DataFrame([input_data])
            input_scaled = st.session_state['scaler'].transform(input_df)
            prediction = st.session_state['model'].predict_proba(input_scaled)[0]
            st.session_state['prediction_value'] = f"{prediction[1]:.2%}"
        else:
            st.warning("è«‹å…ˆè¨“ç·´æ¨¡åž‹ï¼")

    if st.session_state['prediction_value'] is not None:
        st.metric(label="å®¢æˆ¶æµå¤±çš„æ©ŸçŽ‡", value=st.session_state['prediction_value'])
else:
    st.warning("Please train the model first! @data_lemak")

# What-If Analysis
st.header("9. What-If Analysis")
st.write("èª¿æ•´ç‰¹å¾µå€¼ï¼Œçœ‹çœ‹å¦‚ä½•å½±éŸ¿æ¨¡åž‹çš„é æ¸¬ï¼š")

if 'features' in st.session_state:
    feature_to_change = st.selectbox("é¸æ“‡è¦èª¿æ•´çš„ç‰¹å¾µ", st.session_state['features'])
    if feature_to_change in input_data:
        original_value = input_data[feature_to_change]
        new_value = st.slider(f"èª¿æ•´ {feature_to_change} çš„å€¼", 
                            float(st.session_state.df[feature_to_change].min()), 
                            float(st.session_state.df[feature_to_change].max()), 
                            float(original_value))

        if 'original_prediction' not in st.session_state:
            st.session_state['original_prediction'] = None
        if 'new_prediction' not in st.session_state:
            st.session_state['new_prediction'] = None
        if 'change' not in st.session_state:
            st.session_state['change'] = None

        if st.button("æ¯”è¼ƒé æ¸¬çµæžœ"):
            if 'model' in st.session_state:
                original_input = pd.DataFrame([input_data])
                original_scaled = st.session_state['scaler'].transform(original_input)
                st.session_state['original_prediction'] = st.session_state['model'].predict_proba(original_scaled)[0][1]
                
                new_input = original_input.copy()
                new_input[feature_to_change] = new_value
                new_scaled = st.session_state['scaler'].transform(new_input)
                st.session_state['new_prediction'] = st.session_state['model'].predict_proba(new_scaled)[0][1]
                
                st.session_state['change'] = float(st.session_state['new_prediction'] - st.session_state['original_prediction'])
                
        if st.session_state['original_prediction'] is not None and st.session_state['new_prediction'] is not None:
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric(label="åŽŸå§‹é æ¸¬", value=f"{float(st.session_state['original_prediction']):.2%}")
            with col2:
                st.metric(label="æ–°é æ¸¬", value=f"{float(st.session_state['new_prediction']):.2%}")
            with col3:
                if st.session_state['change'] > 0:
                    st.metric(label="è®ŠåŒ–", value=f"{st.session_state['change']:.2%}", delta=f"{st.session_state['change']:.2%}", delta_color="inverse")
                else:
                    st.metric(label="è®ŠåŒ–", value=f"{st.session_state['change']:.2%}", delta=f"{st.session_state['change']:.2%}", delta_color="normal")
else:
    st.warning("Please train the model first! @data_lemak")