# LinkedIn Job Scraper

![Version](https://img.shields.io/badge/version-1.0.0-blue.svg)
![Python Version](https://img.shields.io/badge/python-3.9%2B-green.svg)
![Docker](https://img.shields.io/badge/docker-ready-brightgreen.svg)

A powerful LinkedIn job scraping system for automatically collecting, analyzing, and monitoring job opportunities on LinkedIn.

(ä¸€å€‹å¼·å¤§çš„LinkedInè·ç¼ºçˆ¬èŸ²ç³»çµ±ï¼Œç”¨æ–¼è‡ªå‹•æ”¶é›†ã€åˆ†æå’Œç›£æ§LinkedInä¸Šçš„å·¥ä½œæ©Ÿæœƒã€‚)

## ğŸ“Œ Project Overview | å°ˆæ¡ˆæ¦‚è¿°

The LinkedIn Job Scraper automatically retrieves job information from LinkedIn based on specified keywords and locations, storing them in a database for analysis. This system is particularly useful for job seekers, HR professionals, and market analysts monitoring employment market trends.

(LinkedInè·ç¼ºçˆ¬èŸ²ç³»çµ±èƒ½è‡ªå‹•å¾LinkedInç²å–æŒ‡å®šé—œéµå­—å’Œåœ°é»çš„è·ç¼ºè³‡è¨Šï¼Œä¸¦å­˜å…¥è³‡æ–™åº«ä»¥ä¾›åˆ†æã€‚æœ¬ç³»çµ±ç‰¹åˆ¥é©åˆæ±‚è·è€…ã€äººåŠ›è³‡æºå°ˆæ¥­äººå“¡å’Œå¸‚å ´åˆ†æå¸«ç›£æ§å°±æ¥­å¸‚å ´è¶¨å‹¢ã€‚)

### ğŸŒŸ Core Features | æ ¸å¿ƒåŠŸèƒ½

- **Automated Job Scraping**: Scrape LinkedIn jobs by keyword, location, and time range
  (è‡ªå‹•åŒ–è·ç¼ºçˆ¬å–ï¼šä¾é—œéµå­—ã€åœ°é»å’Œæ™‚é–“ç¯„åœçˆ¬å–LinkedInè·ç¼º)
- **Flexible Search Configurations**: Create and manage multiple search criteria
  (å½ˆæ€§æœå°‹é…ç½®ï¼šå»ºç«‹å’Œç®¡ç†å¤šå€‹æœå°‹æ¢ä»¶ï¼ŒæŒ‰éœ€åŸ·è¡Œ)
- **Scheduled Execution**: Set up timed schedules for automatic execution
  (æ’ç¨‹è‡ªå‹•åŸ·è¡Œï¼šè¨­å®šå®šæ™‚æ’ç¨‹ï¼Œè‡ªå‹•åŸ·è¡Œçˆ¬èŸ²ä»»å‹™)
- **Data Persistence**: Store scraped data in PostgreSQL database, avoiding duplicates
  (è³‡æ–™æŒä¹…åŒ–ï¼šå°‡çˆ¬å–è³‡æ–™å­˜å…¥PostgreSQLè³‡æ–™åº«ï¼Œé¿å…é‡è¤‡)
- **Detailed Job Information**: Collect complete job information including title, company, description
  (è©³ç´°è·ç¼ºè³‡è¨Šï¼šæ”¶é›†è·ç¼ºæ¨™é¡Œã€å…¬å¸ã€æè¿°ã€è³‡æ­·è¦æ±‚ç­‰å®Œæ•´è³‡è¨Š)
- **Docker Containerization**: Simplify deployment and environment management
  (Dockerå®¹å™¨åŒ–ï¼šç°¡åŒ–éƒ¨ç½²å’Œç’°å¢ƒç®¡ç†)
- **Health Monitoring**: System status monitoring and logging
  (å¥åº·ç›£æ§ï¼šç³»çµ±é‹è¡Œç‹€æ…‹ç›£æ§å’Œæ—¥èªŒè¨˜éŒ„)

## ğŸ— System Architecture | ç³»çµ±æ¶æ§‹

```mermaid
graph TD
    User[User/ä½¿ç”¨è€…] --> CLI[Command Line Interface/å‘½ä»¤è¡Œä»‹é¢]
    CLI --> Scheduler[Scheduler/æ’ç¨‹å™¨]
    CLI --> Scraper[Scraper Engine/çˆ¬èŸ²å¼•æ“]
    Scheduler --> Scraper
    Scraper --> Database[(PostgreSQL Database/è³‡æ–™åº«)]
    Setup[Setup Tool/è¨­å®šå·¥å…·] --> Database
    Config[Search Configurations/æœå°‹é…ç½®] <--> Database
    Health[Health Monitoring/å¥åº·ç›£æ§] --> Scheduler
    Health --> Scraper
    Health --> Database
```

## ğŸ“ Project Structure | å°ˆæ¡ˆçµæ§‹

```
linkedin-scraper/
â”œâ”€â”€ app/                  # Application main directory | æ‡‰ç”¨ç¨‹å¼ä¸»ç›®éŒ„
â”‚   â”œâ”€â”€ api/              # API services | APIæœå‹™
â”‚   â”œâ”€â”€ config/           # Configuration settings | é…ç½®è¨­å®š
â”‚   â”œâ”€â”€ database/         # Database models and operations | è³‡æ–™åº«æ¨¡å‹èˆ‡æ“ä½œ
â”‚   â”‚   â”œâ”€â”€ models.py     # Data models | è³‡æ–™æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ operations.py # Database operations | è³‡æ–™åº«æ“ä½œ
â”‚   â”‚   â””â”€â”€ upgrade_db.py # Database upgrade script | è³‡æ–™åº«å‡ç´šè…³æœ¬
â”‚   â”œâ”€â”€ scraper/          # Scraper core components | çˆ¬èŸ²æ ¸å¿ƒçµ„ä»¶
â”‚   â”‚   â”œâ”€â”€ linkedin.py   # LinkedIn scraper implementation | LinkedInçˆ¬èŸ²å¯¦ç¾
â”‚   â”‚   â””â”€â”€ utils.py      # Scraper utility functions | çˆ¬èŸ²å·¥å…·å‡½æ•¸
â”‚   â”œâ”€â”€ monitoring/       # Monitoring components | ç›£æ§çµ„ä»¶
â”‚   â”œâ”€â”€ scheduler/        # Scheduler components | æ’ç¨‹çµ„ä»¶
â”‚   â”œâ”€â”€ utils/            # General utilities | é€šç”¨å·¥å…·
â”‚   â”œâ”€â”€ main.py           # Main program entry | ä¸»ç¨‹å¼å…¥å£
â”‚   â””â”€â”€ setup.py          # Setup script | è¨­ç½®è…³æœ¬
â”œâ”€â”€ docker/               # Docker related files | Dockerç›¸é—œæ–‡ä»¶
â”‚   â”œâ”€â”€ Dockerfile        # Container definition | å®¹å™¨å®šç¾©
â”‚   â””â”€â”€ docker-compose.yml # Container orchestration | å®¹å™¨ç·¨æ’
â”œâ”€â”€ scripts/              # Script files | è…³æœ¬æ–‡ä»¶
â”œâ”€â”€ requirements.txt      # Python dependencies | Pythonä¾è³´
â”œâ”€â”€ migration.md          # GCP migration guide | GCPé·ç§»æŒ‡å—
â””â”€â”€ README.md             # This file | æœ¬æ–‡ä»¶
```

## ğŸš€ Quick Start | å¿«é€Ÿé–‹å§‹

### Prerequisites | å‰ç½®éœ€æ±‚

- Docker and Docker Compose | Dockerèˆ‡Docker Compose
- Python 3.9+ (for local development | æœ¬åœ°é–‹ç™¼)
- PostgreSQL (for local development, included in Docker | æœ¬åœ°é–‹ç™¼ï¼ŒDockerä¸­å·²åŒ…å«)

### Running with Docker (Recommended) | ä½¿ç”¨Dockeré‹è¡Œï¼ˆæ¨è–¦ï¼‰

1. **Clone the project | è¤‡è£½å°ˆæ¡ˆ**
   ```bash
   git clone https://github.com/yourusername/linkedin-scraper.git
   cd linkedin-scraper
   ```

2. **Start Docker container | å•Ÿå‹•Dockerå®¹å™¨**
   ```bash
   docker-compose up -d
   ```

3. **Initialize database | åˆå§‹åŒ–è³‡æ–™åº«**
   ```bash
   docker exec -it linkedin-scraper python -m app.main --init
   ```

4. **Upgrade database** (if adding new fields | å¦‚éœ€æ·»åŠ æ–°æ¬„ä½)
   ```bash
   docker exec -it linkedin-scraper python -m app.main --upgrade
   ```

### Local Development Environment Setup | æœ¬åœ°é–‹ç™¼ç’°å¢ƒè¨­ç½®

1. **Install dependencies | å®‰è£ä¾è³´**
   ```bash
   pip install -r requirements.txt
   ```

2. **Set environment variables | è¨­ç½®ç’°å¢ƒè®Šæ•¸**
   ```bash
   export DATABASE_URL=postgresql://postgres:postgres@localhost:5432/linkedin
   ```

3. **Initialize database | åˆå§‹åŒ–è³‡æ–™åº«**
   ```bash
   python -m app.main --init
   ```

## ğŸ’» Usage Guide | ä½¿ç”¨æŒ‡å—

### 1. Managing Search Configurations | ç®¡ç†æœå°‹é…ç½®

The LinkedIn scraper supports two ways to manage configurations: direct command line and setup.py.

(LinkedInçˆ¬èŸ²æ”¯æŒå…©ç¨®ç®¡ç†é…ç½®çš„æ–¹å¼ï¼šç›´æ¥å‘½ä»¤è¡Œå’Œsetup.pyã€‚)

#### Using setup.py (Recommended) | ä½¿ç”¨setup.pyç®¡ç†ï¼ˆæ¨è–¦ï¼‰

Edit the `default_configs` list in the `app/setup.py` file:

(ç·¨è¼¯`app/setup.py`æ–‡ä»¶çš„`default_configs`åˆ—è¡¨ï¼š)

```python
default_configs = [
    {
        "name": "AI Engineer in Taiwan",
        "keyword": "AI Engineer",
        "location": "Taiwan",
        "time_filter": "r604800",  # Within one week | ä¸€é€±å…§
        "max_pages": 5
    },
    # Add more configurations... | æ·»åŠ æ›´å¤šé…ç½®...
]
```

Then run the following command to update configurations:

(ç„¶å¾Œé‹è¡Œä»¥ä¸‹å‘½ä»¤æ›´æ–°é…ç½®ï¼š)

```bash
docker exec -it linkedin-scraper python -m app.setup --update
```

List all configurations:
(åˆ—å‡ºæ‰€æœ‰é…ç½®ï¼š)
```bash
docker exec -it linkedin-scraper python -m app.setup --list
```

Run specific configurations:
(é‹è¡Œç‰¹å®šé…ç½®ï¼š)
```bash
docker exec -it linkedin-scraper python -m app.setup --run "AI Engineer in Taiwan"
```

#### Using Command Line | ä½¿ç”¨å‘½ä»¤è¡Œç®¡ç†

List all configurations:
(åˆ—å‡ºæ‰€æœ‰é…ç½®ï¼š)
```bash
docker exec -it linkedin-scraper python -m app.main list-configs
```

Add new configuration:
(æ·»åŠ æ–°é…ç½®ï¼š)
```bash
docker exec -it linkedin-scraper python -m app.main add-config --name "Data Scientist" --keyword "Data Scientist" --location "Taiwan" --max-pages 3
```

Update configuration:
(æ›´æ–°é…ç½®ï¼š)
```bash
docker exec -it linkedin-scraper python -m app.main update-config --id 1 --keyword "ML Engineer" --location "Remote"
```

Delete configuration:
(åˆªé™¤é…ç½®ï¼š)
```bash
docker exec -it linkedin-scraper python -m app.main delete-config --id 1
```

### 2. Running the Scraper | åŸ·è¡Œçˆ¬èŸ²

#### Direct Execution for Specific Search | ç›´æ¥åŸ·è¡Œç‰¹å®šæœå°‹

```bash
docker exec -it linkedin-scraper python -m app.main --scrape --keyword="Data Analyst" --location="Taiwan" --max-pages=3
```

#### Run All Enabled Configurations | åŸ·è¡Œæ‰€æœ‰å•Ÿç”¨çš„é…ç½®

```bash
docker exec -it linkedin-scraper python -m app.main run-all-configs
```

#### Start Scheduler for Automatic Execution | å•Ÿå‹•æ’ç¨‹è‡ªå‹•åŸ·è¡Œ

```bash
docker exec -d linkedin-scraper python -m app.main --schedule
```

### 3. Parameter Explanation | åƒæ•¸èªªæ˜

- `--keyword`: Search keyword, e.g., "Data Analyst", "Software Engineer"
  (æœå°‹é—œéµå­—ï¼Œä¾‹å¦‚ "Data Analyst"ã€"Software Engineer")
- `--location`: Search location, e.g., "Taiwan", "Taipei", "Remote"
  (æœå°‹åœ°é»ï¼Œä¾‹å¦‚ "Taiwan"ã€"Taipei"ã€"Remote")
- `--time-filter`: Time filter
  (æ™‚é–“éæ¿¾å™¨)
  - `r86400`: Within 24 hours | 24å°æ™‚å…§
  - `r604800`: Within one week | ä¸€é€±å…§
  - `r2592000`: Within one month | ä¸€å€‹æœˆå…§
- `--max-pages`: Maximum pages to scrape, each page contains about 25 jobs
  (æœ€å¤§çˆ¬å–é æ•¸ï¼Œæ¯é ç´„25å€‹è·ç¼º)

## ğŸ“Š Execution Flow | åŸ·è¡Œæµç¨‹

```mermaid
sequenceDiagram
    participant U as User/ä½¿ç”¨è€…
    participant C as Command Line/å‘½ä»¤è¡Œ
    participant S as Scheduler/æ’ç¨‹å™¨
    participant SC as Scraper Engine/çˆ¬èŸ²å¼•æ“
    participant DB as Database/è³‡æ–™åº«

    Note over U,DB: Initialization Phase/åˆå§‹åŒ–éšæ®µ
    U->>C: Initialize database/åˆå§‹åŒ–è³‡æ–™åº« (--init)
    C->>DB: Create table structure/å‰µå»ºè³‡æ–™è¡¨çµæ§‹
    DB-->>C: Initialization complete/åˆå§‹åŒ–å®Œæˆ
    
    Note over U,DB: Configuration Phase/é…ç½®éšæ®µ
    U->>C: Add search configuration/æ·»åŠ æœç´¢é…ç½®
    C->>DB: Store configuration/å­˜å„²é…ç½®
    
    Note over U,DB: Execution Phase/åŸ·è¡Œéšæ®µ
    U->>C: Run scraper/åŸ·è¡Œçˆ¬èŸ² (--scrape or run-all-configs)
    C->>SC: Start scraping task/å•Ÿå‹•çˆ¬èŸ²ä»»å‹™
    SC->>SC: Scrape LinkedIn search results/çˆ¬å–LinkedInæœç´¢çµæœ
    SC->>SC: Parse job details/è§£æè·ç¼ºè©³æƒ…
    SC->>DB: Store job data/å­˜å„²è·ç¼ºæ•¸æ“š
    
    Note over U,DB: Scheduling Phase/æ’ç¨‹éšæ®µ
    U->>C: Start scheduler/å•Ÿå‹•æ’ç¨‹å™¨ (--schedule)
    C->>S: Initialize scheduler/åˆå§‹åŒ–æ’ç¨‹å™¨
    S->>S: Wait for scheduled time/ç­‰å¾…æ’ç¨‹æ™‚é–“
    S->>SC: Trigger scraping task/è§¸ç™¼çˆ¬èŸ²ä»»å‹™
    SC->>DB: Store new jobs/å­˜å„²æ–°è·ç¼º
    
    Note over U,DB: Query Phase/æŸ¥è©¢éšæ®µ
    U->>DB: Query collected job data/æŸ¥è©¢æ”¶é›†çš„è·ç¼ºæ•¸æ“š
```

## ğŸ” Monitoring System | ç›£æ§ç³»çµ±

### Checking Running Status | æª¢æŸ¥é‹è¡Œç‹€æ…‹

```bash
# Check container status | æŸ¥çœ‹å®¹å™¨ç‹€æ…‹
docker ps | grep linkedin-scraper

# View logs | æŸ¥çœ‹æ—¥èªŒ
docker logs -f linkedin-scraper

# Check recently scraped jobs count | æŸ¥çœ‹æœ€è¿‘çˆ¬å–çš„è·ç¼ºæ•¸é‡
docker exec -it linkedin-scraper python -c "
from app.database.operations import get_db_session
from app.database.models import LinkedInJob
from datetime import datetime, timedelta
import sqlalchemy

session = get_db_session()
yesterday = datetime.now() - timedelta(days=1)
recent_jobs = session.query(LinkedInJob).filter(LinkedInJob.scrape_date >= yesterday).count()
print(f'Jobs scraped in the last 24 hours | éå»24å°æ™‚å…§çˆ¬å–çš„è·ç¼ºæ•¸é‡: {recent_jobs}')
session.close()
"
```

## â˜ï¸ Cloud Deployment | é›²ç«¯éƒ¨ç½²

This system can be deployed to Google Cloud Platform. See the [GCP Migration Guide](migration.md) for detailed steps.

(æœ¬ç³»çµ±å¯ä»¥éƒ¨ç½²åˆ°Google Cloud Platformï¼Œè©³ç´°æ­¥é©Ÿè«‹æŸ¥çœ‹[GCPé·ç§»æŒ‡å—](migration.md)ã€‚)

## âš ï¸ Notes | æ³¨æ„äº‹é …

1. **Scraping Frequency**: Do not set too high a scraping frequency to avoid being restricted by LinkedIn
   (çˆ¬èŸ²åŸ·è¡Œé »ç‡ï¼šè«‹å‹¿è¨­ç½®éé«˜çš„çˆ¬å–é »ç‡ï¼Œä»¥é¿å…è¢«LinkedIné™åˆ¶)
2. **Resource Usage**: The scraper will consume certain CPU and memory resources during execution
   (è³‡æºä½¿ç”¨ï¼šçˆ¬èŸ²åŸ·è¡Œæ™‚æœƒæ¶ˆè€—ä¸€å®šCPUå’Œè¨˜æ†¶é«”è³‡æº)
3. **Network Connection**: Ensure the system has a stable network connection
   (ç¶²çµ¡é€£æ¥ï¼šç¢ºä¿ç³»çµ±æœ‰ç©©å®šçš„ç¶²çµ¡é€£æ¥)
4. **Data Retention**: The system automatically deduplicates data, only adding new jobs without deleting old data
   (è³‡æ–™ä¿ç•™ï¼šç³»çµ±æœƒè‡ªå‹•å»é‡ï¼Œåªæ·»åŠ æ–°è·ç¼ºï¼Œä¸æœƒåˆªé™¤èˆŠè³‡æ–™)

## ğŸ¤ Contribution Guide | è²¢ç»æŒ‡å—

Contributions of code, issue reports, or improvement suggestions are welcome. Please follow these steps:

(æ­¡è¿è²¢ç»ä»£ç¢¼ã€å ±å‘Šå•é¡Œæˆ–æå‡ºæ”¹é€²å»ºè­°ã€‚è«‹éµå¾ªä»¥ä¸‹æ­¥é©Ÿï¼š)

1. Fork the project | Forkå°ˆæ¡ˆ
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License | è¨±å¯è­‰

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

(æœ¬å°ˆæ¡ˆæ¡ç”¨MITè¨±å¯è­‰ - æŸ¥çœ‹[LICENSE](LICENSE)æ–‡ä»¶äº†è§£æ›´å¤šè©³æƒ…ã€‚)